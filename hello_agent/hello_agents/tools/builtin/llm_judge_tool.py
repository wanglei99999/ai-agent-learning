"""
LLM Judge Evaluation Tool

ä½¿ç”¨LLMä½œä¸ºè¯„å§”è¯„ä¼°æ•°æ®ç”Ÿæˆè´¨é‡çš„å·¥å…·
"""

import json
import os
from typing import Dict, Any
from datetime import datetime

from hello_agents.tools.base import Tool
from hello_agents.evaluation.benchmarks.data_generation.dataset import AIDataset
from hello_agents.evaluation.benchmarks.data_generation.llm_judge import LLMJudgeEvaluator
from hello_agents.core.llm import HelloAgentsLLM


class LLMJudgeTool(Tool):
    """LLM Judgeè¯„ä¼°å·¥å…·"""
    
    def __init__(self, llm: HelloAgentsLLM = None):
        """
        åˆå§‹åŒ–LLM Judgeå·¥å…·
        
        Args:
            llm: LLMå®ä¾‹ï¼Œç”¨äºè¯„ä¼°
        """
        super().__init__(
            name="llm_judge_evaluation",
            description="ä½¿ç”¨LLMä½œä¸ºè¯„å§”è¯„ä¼°æ•°æ®ç”Ÿæˆè´¨é‡"
        )
        self.llm = llm
        
    def get_parameters(self) -> Dict[str, Any]:
        """è·å–å·¥å…·å‚æ•°å®šä¹‰"""
        return {
            "type": "object",
            "properties": {
                "generated_data_path": {
                    "type": "string",
                    "description": "ç”Ÿæˆæ•°æ®çš„JSONæ–‡ä»¶è·¯å¾„"
                },
                "reference_data_path": {
                    "type": "string",
                    "description": "å‚è€ƒæ•°æ®çš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºå¯¹æ¯”ï¼‰"
                },
                "reference_year": {
                    "type": "integer",
                    "description": "AIMEçœŸé¢˜å¹´ä»½ï¼ˆå¯é€‰ï¼Œå¦‚2024, 2025ï¼‰"
                },
                "max_samples": {
                    "type": "integer",
                    "description": "æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤è¯„ä¼°æ‰€æœ‰ï¼‰"
                },
                "output_dir": {
                    "type": "string",
                    "description": "è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºevaluation_results/llm_judgeï¼‰"
                },
                "judge_model": {
                    "type": "string",
                    "description": "è¯„å§”æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºgpt-4oï¼‰"
                }
            },
            "required": ["generated_data_path"]
        }
    
    def run(self, params: Dict[str, Any]) -> str:
        """
        è¿è¡ŒLLM Judgeè¯„ä¼°
        
        Args:
            params: å·¥å…·å‚æ•°
        
        Returns:
            è¯„ä¼°ç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        # è§£æå‚æ•°
        generated_data_path = params["generated_data_path"]
        reference_data_path = params.get("reference_data_path")
        reference_year = params.get("reference_year")
        max_samples = params.get("max_samples")
        output_dir = params.get("output_dir", "evaluation_results/llm_judge")
        judge_model = params.get("judge_model", "gpt-4o")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        print("\n" + "="*60)
        print("ğŸ¯ LLM Judgeè¯„ä¼°")
        print("="*60)
        
        # 1. åŠ è½½ç”Ÿæˆæ•°æ®
        print(f"\nğŸ“¥ æ­¥éª¤1: åŠ è½½ç”Ÿæˆæ•°æ®")
        gen_dataset = AIDataset(dataset_type="generated", data_path=generated_data_path)
        gen_problems = gen_dataset.load()
        
        if max_samples:
            gen_problems = gen_problems[:max_samples]
            print(f"   é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°: {max_samples}")
        
        # 2. åŠ è½½å‚è€ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        ref_problems = None
        if reference_data_path:
            print(f"\nğŸ“¥ æ­¥éª¤2: åŠ è½½å‚è€ƒæ•°æ®ï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰")
            ref_dataset = AIDataset(dataset_type="generated", data_path=reference_data_path)
            ref_problems = ref_dataset.load()
        elif reference_year:
            print(f"\nğŸ“¥ æ­¥éª¤2: åŠ è½½å‚è€ƒæ•°æ®ï¼ˆAIME {reference_year}çœŸé¢˜ï¼‰")
            ref_dataset = AIDataset(dataset_type="real", year=reference_year)
            ref_problems = ref_dataset.load()
        else:
            print(f"\nâ­ï¸  æ­¥éª¤2: è·³è¿‡å‚è€ƒæ•°æ®åŠ è½½ï¼ˆæ— å¯¹æ¯”ï¼‰")
        
        # 3. åˆ›å»ºè¯„ä¼°å™¨
        print(f"\nğŸ”§ æ­¥éª¤3: åˆ›å»ºLLM Judgeè¯„ä¼°å™¨")
        evaluator = LLMJudgeEvaluator(llm=self.llm, judge_model=judge_model)
        
        # 4. è¿è¡Œè¯„ä¼°
        print(f"\nğŸš€ æ­¥éª¤4: å¼€å§‹è¯„ä¼°")
        results = evaluator.evaluate_batch(gen_problems, ref_problems)
        
        # 5. ä¿å­˜ç»“æœ
        print(f"\nğŸ’¾ æ­¥éª¤5: ä¿å­˜è¯„ä¼°ç»“æœ")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = os.path.join(output_dir, f"llm_judge_results_{timestamp}.json")
        evaluator.export_results(results, result_file)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        print(f"\nğŸ“Š æ­¥éª¤6: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        report_file = os.path.join(output_dir, f"llm_judge_report_{timestamp}.md")
        self._generate_report(results, report_file)
        
        print("\n" + "="*60)
        print("âœ… LLM Judgeè¯„ä¼°å®Œæˆ")
        print("="*60)
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   - è¯„ä¼°ç»“æœ: {result_file}")
        print(f"   - è¯„ä¼°æŠ¥å‘Š: {report_file}")
        
        # è¿”å›ç®€åŒ–çš„ç»“æœ
        return json.dumps({
            "status": "success",
            "metrics": results["metrics"],
            "num_problems": results["num_problems"],
            "result_file": result_file,
            "report_file": report_file
        }, ensure_ascii=False, indent=2)
    
    def _generate_report(self, results: Dict[str, Any], output_path: str):
        """ç”ŸæˆMarkdownè¯„ä¼°æŠ¥å‘Š"""
        metrics = results["metrics"]
        
        report = f"""# LLM Judgeè¯„ä¼°æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **è¯„ä¼°æ—¥æœŸ**: {results['evaluation_date']}
- **è¯„å§”æ¨¡å‹**: {results['judge_model']}
- **è¯„ä¼°æ•°é‡**: {results['num_problems']} ä¸ªé¢˜ç›®

## è¯„ä¼°ç»“æœ

### æ€»ä½“è¯„åˆ†

- **å¹³å‡æ€»åˆ†**: {metrics['average_total_score']:.2f}/5.0
- **é€šè¿‡ç‡**: {metrics['pass_rate']:.2%} (â‰¥3.5åˆ†)
- **ä¼˜ç§€ç‡**: {metrics['excellent_rate']:.2%} (â‰¥4.5åˆ†)

### å„ç»´åº¦è¯„åˆ†

| ç»´åº¦ | å¹³å‡åˆ† | è¯„çº§ |
|------|--------|------|
| æ­£ç¡®æ€§ (Correctness) | {metrics['dimension_averages']['correctness']:.2f}/5.0 | {self._get_rating(metrics['dimension_averages']['correctness'])} |
| æ¸…æ™°åº¦ (Clarity) | {metrics['dimension_averages']['clarity']:.2f}/5.0 | {self._get_rating(metrics['dimension_averages']['clarity'])} |
| éš¾åº¦åŒ¹é… (Difficulty Match) | {metrics['dimension_averages']['difficulty_match']:.2f}/5.0 | {self._get_rating(metrics['dimension_averages']['difficulty_match'])} |
| å®Œæ•´æ€§ (Completeness) | {metrics['dimension_averages']['completeness']:.2f}/5.0 | {self._get_rating(metrics['dimension_averages']['completeness'])} |

## è¯¦ç»†ç»“æœ

"""
        
        # æ·»åŠ æ¯ä¸ªé¢˜ç›®çš„è¯¦ç»†è¯„åˆ†
        for idx, result in enumerate(results['results'][:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            report += f"""
### é¢˜ç›® {idx + 1}: {result['problem_id']}

- **æ€»åˆ†**: {result['total_score']:.2f}/5.0
- **å„ç»´åº¦è¯„åˆ†**:
  - æ­£ç¡®æ€§: {result['scores']['correctness']:.1f}
  - æ¸…æ™°åº¦: {result['scores']['clarity']:.1f}
  - éš¾åº¦åŒ¹é…: {result['scores']['difficulty_match']:.1f}
  - å®Œæ•´æ€§: {result['scores']['completeness']:.1f}
"""
        
        if len(results['results']) > 10:
            report += f"\n*ï¼ˆä»…æ˜¾ç¤ºå‰10ä¸ªé¢˜ç›®çš„è¯¦ç»†è¯„åˆ†ï¼Œå®Œæ•´ç»“æœè¯·æŸ¥çœ‹JSONæ–‡ä»¶ï¼‰*\n"
        
        report += f"""
## ç»“è®º

åŸºäºLLM Judgeçš„è¯„ä¼°ï¼Œç”Ÿæˆçš„æ•°æ®é›†è´¨é‡{'ä¼˜ç§€' if metrics['average_total_score'] >= 4.5 else 'è‰¯å¥½' if metrics['average_total_score'] >= 3.5 else 'éœ€è¦æ”¹è¿›'}ã€‚

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    def _get_rating(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–è¯„çº§"""
        if score >= 4.5:
            return "ä¼˜ç§€ â­â­â­â­â­"
        elif score >= 4.0:
            return "è‰¯å¥½ â­â­â­â­"
        elif score >= 3.5:
            return "åˆæ ¼ â­â­â­"
        elif score >= 3.0:
            return "ä¸€èˆ¬ â­â­"
        else:
            return "éœ€æ”¹è¿› â­"

