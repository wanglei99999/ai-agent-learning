"""RLè®­ç»ƒå·¥å…·

æä¾›å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬SFTã€GRPOã€PPOç­‰ç®—æ³•ã€‚
"""

from typing import Dict, Any, List, Optional
import json
from pathlib import Path

from ..base import Tool, ToolParameter


class RLTrainingTool(Tool):
    """RLè®­ç»ƒå·¥å…·

    æ”¯æŒçš„è®­ç»ƒç®—æ³•ï¼š
    - SFT: Supervised Fine-Tuning (ç›‘ç£å¾®è°ƒ)
    - GRPO: Group Relative Policy Optimization (ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–)

    æ”¯æŒçš„åŠŸèƒ½ï¼š
    - è®­ç»ƒæ¨¡å‹ (train)
    - åŠ è½½æ•°æ®é›† (load_dataset)
    - åˆ›å»ºå¥–åŠ±å‡½æ•° (create_reward)
    - è¯„ä¼°æ¨¡å‹ (evaluate)
    """

    def __init__(self):
        super().__init__(
            name="rl_training",
            description=(
                "å¼ºåŒ–å­¦ä¹ è®­ç»ƒå·¥å…·ã€‚æ”¯æŒSFTã€GRPOç­‰ç®—æ³•ï¼Œ"
                "ç”¨äºè®­ç»ƒå’Œä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚"
                "ä¹Ÿæ”¯æŒæ•°æ®é›†åŠ è½½ã€å¥–åŠ±å‡½æ•°åˆ›å»ºã€æ¨¡å‹è¯„ä¼°ç­‰åŠŸèƒ½ã€‚"
                "æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†å’Œå¥–åŠ±å‡½æ•°ã€‚"
            )
        )

        # æ£€æŸ¥TRLæ˜¯å¦å¯ç”¨
        try:
            from hello_agents.rl import TRL_AVAILABLE
            self.trl_available = TRL_AVAILABLE
        except ImportError:
            self.trl_available = False

        # å­˜å‚¨è‡ªå®šä¹‰æ•°æ®é›†å’Œå¥–åŠ±å‡½æ•°
        self.custom_datasets = {}
        self.custom_reward_functions = {}

    def register_dataset(self, name: str, dataset) -> None:
        """
        æ³¨å†Œè‡ªå®šä¹‰æ•°æ®é›†

        Args:
            name: æ•°æ®é›†åç§°
            dataset: æ•°æ®é›†å¯¹è±¡(HuggingFace Dataset)
        """
        self.custom_datasets[name] = dataset
        print(f"âœ… å·²æ³¨å†Œè‡ªå®šä¹‰æ•°æ®é›†: {name}")

    def register_reward_function(self, name: str, reward_fn) -> None:
        """
        æ³¨å†Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

        Args:
            name: å¥–åŠ±å‡½æ•°åç§°
            reward_fn: å¥–åŠ±å‡½æ•°(æ¥å—completionså’Œkwargs,è¿”å›rewardsåˆ—è¡¨)
        """
        self.custom_reward_functions[name] = reward_fn
        print(f"âœ… å·²æ³¨å†Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°: {name}")

    def run(self, parameters: Dict[str, Any]) -> str:
        """
        æ‰§è¡ŒRLç›¸å…³æ“ä½œ

        Args:
            parameters: æ“ä½œå‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
                - action: æ“ä½œç±»å‹ ("train", "load_dataset", "create_reward", "evaluate")

                è®­ç»ƒå‚æ•° (action="train"):
                - algorithm: è®­ç»ƒç®—æ³• ("sft", "grpo")
                - model_name: æ¨¡å‹åç§°ï¼ˆé»˜è®¤: "Qwen/Qwen2-0.5B-Instruct"ï¼‰
                - dataset: æ•°æ®é›†åç§°ï¼ˆé»˜è®¤: "gsm8k"ï¼‰
                - max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
                - num_epochs: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 3ï¼‰
                - output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: "./output"ï¼‰
                - use_lora: æ˜¯å¦ä½¿ç”¨LoRAï¼ˆé»˜è®¤: Trueï¼‰
                - batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 4ï¼‰

                æ•°æ®é›†åŠ è½½å‚æ•° (action="load_dataset"):
                - format: æ•°æ®æ ¼å¼ ("sft", "rl")
                - split: æ•°æ®é›†åˆ’åˆ† ("train", "test")
                - max_samples: æœ€å¤§æ ·æœ¬æ•°

                å¥–åŠ±å‡½æ•°å‚æ•° (action="create_reward"):
                - reward_type: å¥–åŠ±ç±»å‹ ("accuracy", "length_penalty", "step")
                - penalty_weight: é•¿åº¦æƒ©ç½šæƒé‡ï¼ˆä»…length_penaltyï¼‰
                - step_bonus: æ­¥éª¤å¥–åŠ±ï¼ˆä»…stepï¼‰

        Returns:
            æ“ä½œç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        # æ£€æŸ¥TRLæ˜¯å¦å¯ç”¨
        if not self.trl_available:
            return json.dumps({
                "status": "error",
                "message": (
                    "TRLæœªå®‰è£…ã€‚è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                    "pip install hello-agents[rl]\n"
                    "æˆ–\n"
                    "pip install trl"
                )
            }, ensure_ascii=False, indent=2)

        # è·å–æ“ä½œç±»å‹
        action = parameters.get("action", "train").lower()

        try:
            if action == "train":
                return self._handle_train(parameters)
            elif action == "load_dataset":
                return self._handle_load_dataset(parameters)
            elif action == "create_reward":
                return self._handle_create_reward(parameters)
            elif action == "evaluate":
                return self._handle_evaluate(parameters)
            else:
                result = {
                    "status": "error",
                    "message": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}ã€‚æ”¯æŒçš„æ“ä½œ: train, load_dataset, create_reward, evaluate"
                }
                return json.dumps(result, ensure_ascii=False, indent=2)
        except Exception as e:
            import traceback
            error_result = {
                "status": "error",
                "message": f"æ“ä½œå¤±è´¥: {str(e)}",
                "traceback": traceback.format_exc()
            }
            return json.dumps(error_result, ensure_ascii=False, indent=2)

    def _handle_train(self, parameters: Dict[str, Any]) -> str:
        """å¤„ç†è®­ç»ƒæ“ä½œ"""
        algorithm = parameters.get("algorithm", "sft").lower()
        model_name = parameters.get("model_name", "Qwen/Qwen2-0.5B-Instruct")
        dataset_name = parameters.get("dataset", "gsm8k")
        max_samples = parameters.get("max_samples", None)
        num_epochs = parameters.get("num_epochs", 3)
        output_dir = parameters.get("output_dir", "./output")
        use_lora = parameters.get("use_lora", True)
        batch_size = parameters.get("batch_size", 4)

        # æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†
        custom_dataset = parameters.get("custom_dataset", None)
        # æ”¯æŒè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
        custom_reward = parameters.get("custom_reward", None)

        # æ”¯æŒè®­ç»ƒç›‘æ§é…ç½®
        use_wandb = parameters.get("use_wandb", False)
        use_tensorboard = parameters.get("use_tensorboard", True)
        wandb_project = parameters.get("wandb_project", None)

        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹ {algorithm.upper()} è®­ç»ƒ")
        print(f"{'='*60}")
        print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
        if custom_dataset:
            print(f"ğŸ“Š æ•°æ®é›†: è‡ªå®šä¹‰æ•°æ®é›†")
        else:
            print(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
        print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¯ ç®—æ³•: {algorithm.upper()}")
        if custom_reward:
            print(f"ğŸ å¥–åŠ±å‡½æ•°: è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°")

        # æ‰“å°ç›‘æ§é…ç½®
        monitoring = []
        if use_wandb:
            monitoring.append(f"wandb (é¡¹ç›®: {wandb_project or 'default'})")
        if use_tensorboard:
            monitoring.append("tensorboard")
        if monitoring:
            print(f"ğŸ“Š è®­ç»ƒç›‘æ§: {', '.join(monitoring)}")

        print(f"{'='*60}\n")

        if algorithm == "sft":
            result = self._train_sft(
                model_name=model_name,
                dataset_name=dataset_name,
                max_samples=max_samples,
                num_epochs=num_epochs,
                output_dir=output_dir,
                use_lora=use_lora,
                batch_size=batch_size,
                custom_dataset=custom_dataset,
                use_wandb=use_wandb,
                use_tensorboard=use_tensorboard,
                wandb_project=wandb_project
            )
        elif algorithm == "grpo":
            result = self._train_grpo(
                model_name=model_name,
                dataset_name=dataset_name,
                max_samples=max_samples,
                num_epochs=num_epochs,
                output_dir=output_dir,
                use_lora=use_lora,
                batch_size=batch_size,
                custom_dataset=custom_dataset,
                custom_reward=custom_reward,
                use_wandb=use_wandb,
                use_tensorboard=use_tensorboard,
                wandb_project=wandb_project
            )
        else:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}ã€‚æ”¯æŒçš„ç®—æ³•: sft, grpo"
            }

        return json.dumps(result, ensure_ascii=False, indent=2)

    def _handle_load_dataset(self, parameters: Dict[str, Any]) -> str:
        """å¤„ç†æ•°æ®é›†åŠ è½½æ“ä½œ"""
        from hello_agents.rl import create_sft_dataset, create_rl_dataset

        format_type = parameters.get("format", "sft").lower()
        split = parameters.get("split", "train")
        max_samples = parameters.get("max_samples", 100)
        model_name = parameters.get("model_name", "Qwen/Qwen3-0.6B")

        if format_type == "sft":
            dataset = create_sft_dataset(split=split, max_samples=max_samples)
        elif format_type == "rl":
            dataset = create_rl_dataset(split=split, max_samples=max_samples, model_name=model_name)
        else:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {format_type}ã€‚æ”¯æŒçš„æ ¼å¼: sft, rl"
            }, ensure_ascii=False, indent=2)

        result = {
            "status": "success",
            "format": format_type,
            "split": split,
            "dataset_size": len(dataset),
            "sample_keys": list(dataset[0].keys()) if len(dataset) > 0 else []
        }
        return json.dumps(result, ensure_ascii=False, indent=2)

    def _handle_create_reward(self, parameters: Dict[str, Any]) -> str:
        """å¤„ç†å¥–åŠ±å‡½æ•°åˆ›å»ºæ“ä½œ"""
        from hello_agents.rl import (
            create_accuracy_reward,
            create_length_penalty_reward,
            create_step_reward
        )

        reward_type = parameters.get("reward_type", "accuracy").lower()

        if reward_type == "accuracy":
            reward_fn = create_accuracy_reward()
            result = {
                "status": "success",
                "reward_type": "accuracy",
                "description": "å‡†ç¡®æ€§å¥–åŠ±å‡½æ•°: ç­”æ¡ˆæ­£ç¡®=1.0, é”™è¯¯=0.0"
            }
        elif reward_type == "length_penalty":
            penalty_weight = parameters.get("penalty_weight", 0.001)
            max_length = parameters.get("max_length", 1024)
            # åˆ›å»ºåŸºç¡€å¥–åŠ±å‡½æ•°
            base_reward_fn = create_accuracy_reward()
            reward_fn = create_length_penalty_reward(
                base_reward_fn=base_reward_fn,
                max_length=max_length,
                penalty_weight=penalty_weight
            )
            result = {
                "status": "success",
                "reward_type": "length_penalty",
                "penalty_weight": penalty_weight,
                "max_length": max_length,
                "description": f"é•¿åº¦æƒ©ç½šå¥–åŠ±å‡½æ•°: åŸºç¡€å¥–åŠ± - {penalty_weight} * (é•¿åº¦ / {max_length})"
            }
        elif reward_type == "step":
            step_bonus = parameters.get("step_bonus", 0.1)
            # åˆ›å»ºåŸºç¡€å¥–åŠ±å‡½æ•°
            base_reward_fn = create_accuracy_reward()
            reward_fn = create_step_reward(
                base_reward_fn=base_reward_fn,
                step_bonus=step_bonus
            )
            result = {
                "status": "success",
                "reward_type": "step",
                "step_bonus": step_bonus,
                "description": f"æ­¥éª¤å¥–åŠ±å‡½æ•°: åŸºç¡€å¥–åŠ± + {step_bonus} * æ­¥éª¤æ•°"
            }
        else:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„å¥–åŠ±ç±»å‹: {reward_type}ã€‚æ”¯æŒçš„ç±»å‹: accuracy, length_penalty, step"
            }, ensure_ascii=False, indent=2)

        return json.dumps(result, ensure_ascii=False, indent=2)

    def _handle_evaluate(self, parameters: Dict[str, Any]) -> str:
        """å¤„ç†æ¨¡å‹è¯„ä¼°æ“ä½œ"""
        try:
            from hello_agents.rl import (
                create_rl_dataset,
                create_accuracy_reward,
                evaluate_rewards
            )
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            model_path = parameters.get("model_path")
            max_samples = parameters.get("max_samples", 100)

            if not model_path:
                return json.dumps({
                    "status": "error",
                    "message": "ç¼ºå°‘å¿…éœ€å‚æ•°: model_path"
                }, ensure_ascii=False, indent=2)

            # åŠ è½½æµ‹è¯•æ•°æ®
            print(f"ğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ®é›† (max_samples={max_samples})...")
            dataset = create_rl_dataset(split="test", max_samples=max_samples, model_name=model_path)

            # åŠ è½½æ¨¡å‹å’Œtokenizer
            print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}...")
            try:
                model = AutoModelForCausalLM.from_pretrained(model_path)
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                device = "cuda" if torch.cuda.is_available() else "cpu"
                model = model.to(device)
                model.eval()
            except Exception as e:
                return json.dumps({
                    "status": "error",
                    "message": f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
                }, ensure_ascii=False, indent=2)

            # ç”Ÿæˆé¢„æµ‹
            print("ğŸ”® ç”Ÿæˆé¢„æµ‹...")
            completions = []
            ground_truths = []

            # å¯¼å…¥tqdmç”¨äºè¿›åº¦æ¡
            try:
                from tqdm import tqdm
                use_tqdm = True
            except ImportError:
                use_tqdm = False
                print("  æç¤º: å®‰è£…tqdmå¯æ˜¾ç¤ºè¿›åº¦æ¡ (pip install tqdm)")

            # åˆ›å»ºè¿­ä»£å™¨
            iterator = range(min(max_samples, len(dataset)))
            if use_tqdm:
                iterator = tqdm(iterator, desc="  è¯„ä¼°è¿›åº¦", unit="æ ·æœ¬")

            for i in iterator:
                prompt = dataset[i]["prompt"]
                ground_truth = dataset[i]["ground_truth"]

                # ç”Ÿæˆå›ç­”
                inputs = tokenizer(prompt, return_tensors="pt").to(device)
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=128,  # å‡å°‘ç”Ÿæˆé•¿åº¦åŠ å¿«é€Ÿåº¦
                        temperature=0.7,
                        do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç åŠ å¿«é€Ÿåº¦
                        pad_token_id=tokenizer.pad_token_id
                    )
                # åªå–ç”Ÿæˆçš„éƒ¨åˆ†,ä¸åŒ…æ‹¬è¾“å…¥
                completion = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

                completions.append(completion)
                ground_truths.append(ground_truth)

                # å¦‚æœæ²¡æœ‰tqdm,æ¯10ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
                if not use_tqdm and (i + 1) % 10 == 0:
                    print(f"  è¿›åº¦: {i+1}/{max_samples}")

            # è®¡ç®—å¥–åŠ±
            print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
            reward_fn = create_accuracy_reward()
            rewards = reward_fn(completions, ground_truth=ground_truths)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_reward = sum(rewards) / len(rewards)
            accuracy = avg_reward  # å¯¹äºå‡†ç¡®æ€§å¥–åŠ±,å¹³å‡å¥–åŠ±å°±æ˜¯å‡†ç¡®ç‡

            result = {
                "status": "success",
                "model_path": model_path,
                "num_samples": len(completions),
                "accuracy": f"{accuracy:.2%}",
                "average_reward": f"{avg_reward:.4f}",
                "device": device
            }

            print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
            print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")

            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            return json.dumps({
                "status": "error",
                "message": f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)
    
    def _train_sft(
        self,
        model_name: str,
        dataset_name: str,
        max_samples: Optional[int],
        num_epochs: int,
        output_dir: str,
        use_lora: bool,
        batch_size: int,
        custom_dataset = None,
        use_wandb: bool = False,
        use_tensorboard: bool = True,
        wandb_project: Optional[str] = None
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒSFTè®­ç»ƒ"""
        from hello_agents.rl import (
            SFTTrainerWrapper,
            TrainingConfig,
            create_sft_dataset,
            setup_training_environment
        )

        # åˆ›å»ºé…ç½®
        config = TrainingConfig(
            model_name=model_name,
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            use_lora=use_lora,
            use_wandb=use_wandb,
            use_tensorboard=use_tensorboard,
            wandb_project=wandb_project
        )

        # è®¾ç½®ç¯å¢ƒ
        setup_training_environment(config)

        # åŠ è½½æ•°æ®é›†
        if custom_dataset is not None:
            # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
            dataset = custom_dataset
            print(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†: {len(dataset)} ä¸ªæ ·æœ¬")
        elif dataset_name in self.custom_datasets:
            # ä½¿ç”¨æ³¨å†Œçš„è‡ªå®šä¹‰æ•°æ®é›†
            dataset = self.custom_datasets[dataset_name]
            print(f"âœ… ä½¿ç”¨æ³¨å†Œçš„æ•°æ®é›† '{dataset_name}': {len(dataset)} ä¸ªæ ·æœ¬")
        else:
            # ä½¿ç”¨é»˜è®¤æ•°æ®é›†
            dataset = create_sft_dataset(max_samples=max_samples)

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer_wrapper = SFTTrainerWrapper(config=config, dataset=dataset)

        # å¼€å§‹è®­ç»ƒ
        trainer_wrapper.train()

        # ä¿å­˜æ¨¡å‹
        trainer_wrapper.save_model()

        return {
            "status": "success",
            "algorithm": "SFT",
            "model": model_name,
            "output_dir": output_dir,
            "num_epochs": num_epochs,
            "dataset_size": len(dataset)
        }
    
    def _train_grpo(
        self,
        model_name: str,
        dataset_name: str,
        max_samples: Optional[int],
        num_epochs: int,
        output_dir: str,
        use_lora: bool,
        batch_size: int,
        custom_dataset = None,
        custom_reward = None,
        use_wandb: bool = False,
        use_tensorboard: bool = True,
        wandb_project: Optional[str] = None
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒGRPOè®­ç»ƒ"""
        from hello_agents.rl import (
            GRPOTrainerWrapper,
            TrainingConfig,
            create_rl_dataset,
            create_accuracy_reward,
            setup_training_environment
        )

        # åˆ›å»ºé…ç½®
        config = TrainingConfig(
            model_name=model_name,
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            use_lora=use_lora,
            use_wandb=use_wandb,
            use_tensorboard=use_tensorboard,
            wandb_project=wandb_project
        )

        # è®¾ç½®ç¯å¢ƒ
        setup_training_environment(config)

        # åŠ è½½æ•°æ®é›†
        if custom_dataset is not None:
            # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
            dataset = custom_dataset
            print(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†: {len(dataset)} ä¸ªæ ·æœ¬")
        elif dataset_name in self.custom_datasets:
            # ä½¿ç”¨æ³¨å†Œçš„è‡ªå®šä¹‰æ•°æ®é›†
            dataset = self.custom_datasets[dataset_name]
            print(f"âœ… ä½¿ç”¨æ³¨å†Œçš„æ•°æ®é›† '{dataset_name}': {len(dataset)} ä¸ªæ ·æœ¬")
        else:
            # ä½¿ç”¨é»˜è®¤æ•°æ®é›†
            dataset = create_rl_dataset(max_samples=max_samples, model_name=model_name)

        # åˆ›å»ºå¥–åŠ±å‡½æ•°
        if custom_reward is not None:
            # ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
            reward_fn = custom_reward
            print(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°")
        elif dataset_name in self.custom_reward_functions:
            # ä½¿ç”¨æ³¨å†Œçš„å¥–åŠ±å‡½æ•°
            reward_fn = self.custom_reward_functions[dataset_name]
            print(f"âœ… ä½¿ç”¨æ³¨å†Œçš„å¥–åŠ±å‡½æ•° '{dataset_name}'")
        else:
            # ä½¿ç”¨é»˜è®¤å¥–åŠ±å‡½æ•°
            reward_fn = create_accuracy_reward()

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer_wrapper = GRPOTrainerWrapper(
            config=config,
            dataset=dataset,
            reward_fn=reward_fn
        )

        # å¼€å§‹è®­ç»ƒ
        trainer_wrapper.train()

        # ä¿å­˜æ¨¡å‹
        trainer_wrapper.save_model()

        return {
            "status": "success",
            "algorithm": "GRPO",
            "model": model_name,
            "output_dir": output_dir,
            "num_epochs": num_epochs,
            "dataset_size": len(dataset)
        }
    
    def get_parameters(self) -> List[ToolParameter]:
        """è·å–å·¥å…·å‚æ•°å®šä¹‰"""
        return [
            ToolParameter(
                name="action",
                type="string",
                description="æ“ä½œç±»å‹: train (è®­ç»ƒ), load_dataset (åŠ è½½æ•°æ®é›†), create_reward (åˆ›å»ºå¥–åŠ±å‡½æ•°), evaluate (è¯„ä¼°æ¨¡å‹)",
                required=False,
                default="train"
            ),
            ToolParameter(
                name="algorithm",
                type="string",
                description="è®­ç»ƒç®—æ³• (ä»…train): sft (ç›‘ç£å¾®è°ƒ), grpo (ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–)",
                required=False,
                default="sft"
            ),
            ToolParameter(
                name="model_name",
                type="string",
                description="æ¨¡å‹åç§° (ä»…train)ï¼Œä¾‹å¦‚: Qwen/Qwen2-0.5B-Instruct",
                required=False,
                default="Qwen/Qwen2-0.5B-Instruct"
            ),
            ToolParameter(
                name="dataset",
                type="string",
                description="æ•°æ®é›†åç§° (ä»…train)ï¼Œç›®å‰æ”¯æŒ: gsm8k",
                required=False,
                default="gsm8k"
            ),
            ToolParameter(
                name="format",
                type="string",
                description="æ•°æ®æ ¼å¼ (ä»…load_dataset): sft, rl",
                required=False,
                default="sft"
            ),
            ToolParameter(
                name="split",
                type="string",
                description="æ•°æ®é›†åˆ’åˆ† (ä»…load_dataset): train, test",
                required=False,
                default="train"
            ),
            ToolParameter(
                name="reward_type",
                type="string",
                description="å¥–åŠ±ç±»å‹ (ä»…create_reward): accuracy, length_penalty, step",
                required=False,
                default="accuracy"
            ),
            ToolParameter(
                name="max_samples",
                type="integer",
                description="æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®",
                required=False,
                default=None
            ),
            ToolParameter(
                name="num_epochs",
                type="integer",
                description="è®­ç»ƒè½®æ•° (ä»…train)",
                required=False,
                default=3
            ),
            ToolParameter(
                name="output_dir",
                type="string",
                description="è¾“å‡ºç›®å½• (ä»…train)",
                required=False,
                default="./output"
            ),
            ToolParameter(
                name="use_lora",
                type="boolean",
                description="æ˜¯å¦ä½¿ç”¨LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ (ä»…train)",
                required=False,
                default=True
            ),
            ToolParameter(
                name="batch_size",
                type="integer",
                description="æ‰¹æ¬¡å¤§å° (ä»…train)",
                required=False,
                default=4
            ),
        ]


# ä¾¿æ·å‡½æ•°
def train_with_sft(
    model_name: str = "Qwen/Qwen2-0.5B-Instruct",
    max_samples: Optional[int] = 100,
    num_epochs: int = 3,
    output_dir: str = "./output/sft"
) -> str:
    """
    ä½¿ç”¨SFTè®­ç»ƒæ¨¡å‹ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        num_epochs: è®­ç»ƒè½®æ•°
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        è®­ç»ƒç»“æœJSONå­—ç¬¦ä¸²
    """
    tool = RLTrainingTool()
    return tool.run({
        "action": "train",
        "algorithm": "sft",
        "model_name": model_name,
        "max_samples": max_samples,
        "num_epochs": num_epochs,
        "output_dir": output_dir
    })


def train_with_grpo(
    model_name: str = "Qwen/Qwen2-0.5B-Instruct",
    max_samples: Optional[int] = 100,
    num_epochs: int = 3,
    output_dir: str = "./output/grpo"
) -> str:
    """
    ä½¿ç”¨GRPOè®­ç»ƒæ¨¡å‹ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        num_epochs: è®­ç»ƒè½®æ•°
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        è®­ç»ƒç»“æœJSONå­—ç¬¦ä¸²
    """
    tool = RLTrainingTool()
    return tool.run({
        "action": "train",
        "algorithm": "grpo",
        "model_name": model_name,
        "max_samples": max_samples,
        "num_epochs": num_epochs,
        "output_dir": output_dir
    })


def load_dataset(
    format_type: str = "sft",
    split: str = "train",
    max_samples: int = 100
) -> str:
    """
    åŠ è½½æ•°æ®é›†ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        format_type: æ•°æ®æ ¼å¼ ("sft", "rl")
        split: æ•°æ®é›†åˆ’åˆ† ("train", "test")
        max_samples: æœ€å¤§æ ·æœ¬æ•°

    Returns:
        æ•°æ®é›†ä¿¡æ¯JSONå­—ç¬¦ä¸²
    """
    tool = RLTrainingTool()
    return tool.run({
        "action": "load_dataset",
        "format": format_type,
        "split": split,
        "max_samples": max_samples
    })


def create_reward_function(
    reward_type: str = "accuracy",
    **kwargs
) -> str:
    """
    åˆ›å»ºå¥–åŠ±å‡½æ•°ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        reward_type: å¥–åŠ±ç±»å‹ ("accuracy", "length_penalty", "step")
        **kwargs: å…¶ä»–å‚æ•°
            - penalty_weight: é•¿åº¦æƒ©ç½šæƒé‡ï¼ˆä»…length_penaltyï¼‰
            - step_bonus: æ­¥éª¤å¥–åŠ±ï¼ˆä»…stepï¼‰

    Returns:
        å¥–åŠ±å‡½æ•°ä¿¡æ¯JSONå­—ç¬¦ä¸²
    """
    tool = RLTrainingTool()
    params = {
        "action": "create_reward",
        "reward_type": reward_type
    }
    params.update(kwargs)
    return tool.run(params)


def evaluate_model(
    model_path: str,
    max_samples: int = 100
) -> str:
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        max_samples: è¯„ä¼°æ ·æœ¬æ•°

    Returns:
        è¯„ä¼°ç»“æœJSONå­—ç¬¦ä¸²
    """
    tool = RLTrainingTool()
    return tool.run({
        "action": "evaluate",
        "model_path": model_path,
        "max_samples": max_samples
    })
