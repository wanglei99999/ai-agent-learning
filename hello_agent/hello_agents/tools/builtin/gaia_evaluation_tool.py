"""GAIAè¯„ä¼°å·¥å…·

GAIA (General AI Assistants) è¯„ä¼°å·¥å…·
ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„é€šç”¨èƒ½åŠ›
"""

from typing import Dict, Any, List, Optional, Union
from pathlib import Path
import json
from datetime import datetime
from ..base import Tool, ToolParameter
from hello_agents.evaluation.benchmarks.gaia.dataset import GAIADataset
from hello_agents.evaluation.benchmarks.gaia.evaluator import GAIAEvaluator
from hello_agents.evaluation.benchmarks.gaia.metrics import GAIAMetrics


class GAIAEvaluationTool(Tool):
    """GAIAè¯„ä¼°å·¥å…·
    
    ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„é€šç”¨AIåŠ©æ‰‹èƒ½åŠ›ã€‚
    æ”¯æŒä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼š
    - Level 1: ç®€å•ä»»åŠ¡ï¼ˆ0æ­¥æ¨ç†ï¼‰
    - Level 2: ä¸­ç­‰ä»»åŠ¡ï¼ˆ1-5æ­¥æ¨ç†ï¼‰
    - Level 3: å›°éš¾ä»»åŠ¡ï¼ˆ5+æ­¥æ¨ç†ï¼‰
    """
    
    def __init__(self, local_data_path: Optional[str] = None):
        """åˆå§‹åŒ–GAIAè¯„ä¼°å·¥å…·
        
        Args:
            local_data_path: æœ¬åœ°æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        super().__init__(
            name="gaia_evaluation",
            description=(
                "è¯„ä¼°æ™ºèƒ½ä½“çš„é€šç”¨AIåŠ©æ‰‹èƒ½åŠ›ã€‚ä½¿ç”¨GAIA (General AI Assistants)åŸºå‡†æµ‹è¯•ã€‚"
                "æ”¯æŒä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼šLevel 1(ç®€å•)ã€Level 2(ä¸­ç­‰)ã€Level 3(å›°éš¾)ã€‚"
            )
        )
        self.local_data_path = local_data_path
        self.dataset = None
        self.evaluator = None
        self.metrics_calculator = GAIAMetrics()
    
    def get_parameters(self) -> List[ToolParameter]:
        """è·å–å·¥å…·å‚æ•°å®šä¹‰"""
        return [
            ToolParameter(
                name="agent",
                type="object",
                description="è¦è¯„ä¼°çš„æ™ºèƒ½ä½“å®ä¾‹",
                required=True
            ),
            ToolParameter(
                name="level",
                type="integer",
                description="éš¾åº¦çº§åˆ«ï¼š1(ç®€å•), 2(ä¸­ç­‰), 3(å›°éš¾), None(å…¨éƒ¨)",
                required=False,
                default=None
            ),
            ToolParameter(
                name="max_samples",
                type="integer",
                description="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨",
                required=False,
                default=None
            ),
            ToolParameter(
                name="local_data_dir",
                type="string",
                description="æœ¬åœ°æ•°æ®é›†ç›®å½•è·¯å¾„",
                required=False,
                default=None
            )
        ]
    
    def run(
        self,
        agent: Any,
        level: Optional[int] = None,
        max_samples: Optional[int] = None,
        local_data_dir: Optional[str] = None,
        export_results: bool = True,
        generate_report: bool = True
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒGAIAä¸€é”®è¯„ä¼°

        Args:
            agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“
            level: éš¾åº¦çº§åˆ« (1-3)ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            local_data_dir: æœ¬åœ°æ•°æ®ç›®å½•è·¯å¾„
            export_results: æ˜¯å¦å¯¼å‡ºGAIAæ ¼å¼ç»“æœ
            generate_report: æ˜¯å¦ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\n" + "=" * 60)
        print("GAIAä¸€é”®è¯„ä¼°")
        print("=" * 60)

        # æ˜¾ç¤ºé…ç½®
        print(f"\né…ç½®:")
        print(f"   æ™ºèƒ½ä½“: {getattr(agent, 'name', 'Unknown')}")
        print(f"   éš¾åº¦çº§åˆ«: {level or 'å…¨éƒ¨'}")
        print(f"   æ ·æœ¬æ•°é‡: {max_samples or 'å…¨éƒ¨'}")

        try:
            # æ­¥éª¤1: è¿è¡ŒHelloAgentsè¯„ä¼°
            print("\n" + "=" * 60)
            print("æ­¥éª¤1: è¿è¡ŒHelloAgentsè¯„ä¼°")
            print("=" * 60)

            results = self._run_evaluation(agent, level, max_samples, local_data_dir)

            # æ­¥éª¤2: å¯¼å‡ºGAIAæ ¼å¼ç»“æœ
            if export_results:
                print("\n" + "=" * 60)
                print("æ­¥éª¤2: å¯¼å‡ºGAIAæ ¼å¼ç»“æœ")
                print("=" * 60)

                self._export_results(results)

            # æ­¥éª¤3: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
            if generate_report:
                print("\n" + "=" * 60)
                print("æ­¥éª¤3: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
                print("=" * 60)

                self.generate_report(results)

            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            print("\n" + "=" * 60)
            print("ğŸ¯ æœ€ç»ˆç»“æœ")
            print("=" * 60)
            print(f"   ç²¾ç¡®åŒ¹é…ç‡: {results['exact_match_rate']:.2%}")
            print(f"   éƒ¨åˆ†åŒ¹é…ç‡: {results['partial_match_rate']:.2%}")
            print(f"   æ­£ç¡®æ•°: {results['exact_matches']}/{results['total_samples']}")

            return results

        except Exception as e:
            print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "benchmark": "GAIA",
                "agent_name": getattr(agent, 'name', 'Unknown')
            }

    def _run_evaluation(
        self,
        agent: Any,
        level: Optional[int],
        max_samples: Optional[int],
        local_data_dir: Optional[str]
    ) -> Dict[str, Any]:
        """è¿è¡Œè¯„ä¼°"""
        # åŠ è½½æ•°æ®é›†
        self.dataset = GAIADataset(
            level=level,
            local_data_dir=local_data_dir or self.local_data_path
        )
        dataset_items = self.dataset.load()

        if not dataset_items:
            raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥æˆ–ä¸ºç©º")

        # åˆ›å»ºè¯„ä¼°å™¨
        self.evaluator = GAIAEvaluator(
            dataset=self.dataset,
            level=level,
            local_data_dir=local_data_dir or self.local_data_path
        )

        # è¿è¡Œè¯„ä¼°
        results = self.evaluator.evaluate(agent, max_samples)

        return results

    def _export_results(self, results: Dict[str, Any]) -> None:
        """å¯¼å‡ºGAIAæ ¼å¼ç»“æœå’Œæäº¤è¯´æ˜"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("./evaluation_results/gaia_official")
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        agent_name = results.get("agent_name", "Unknown").replace("/", "_")
        level = results.get("level_filter")
        level_str = f"_level{level}" if level else "_all"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"gaia{level_str}_result_{timestamp}.jsonl"

        # å¯¼å‡ºJSONLç»“æœ
        self.evaluator.export_to_gaia_format(
            results,
            output_file,
            include_reasoning=True
        )

        # ç”Ÿæˆæäº¤è¯´æ˜æ–‡ä»¶
        self._generate_submission_guide(results, output_dir, output_file)

    def _generate_submission_guide(
        self,
        results: Dict[str, Any],
        output_dir: Path,
        result_file: Path
    ) -> None:
        """ç”Ÿæˆæäº¤è¯´æ˜æ–‡ä»¶

        Args:
            results: è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
            result_file: ç»“æœæ–‡ä»¶è·¯å¾„
        """
        agent_name = results.get("agent_name", "Unknown")
        level = results.get("level_filter")
        total_samples = results.get("total_samples", 0)
        exact_matches = results.get("exact_matches", 0)
        exact_match_rate = results.get("exact_match_rate", 0)

        # ç”Ÿæˆæäº¤è¯´æ˜
        guide_content = f"""# GAIAè¯„ä¼°ç»“æœæäº¤æŒ‡å—

## ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦

- **æ¨¡å‹åç§°**: {agent_name}
- **è¯„ä¼°çº§åˆ«**: {level or 'å…¨éƒ¨'}
- **æ€»æ ·æœ¬æ•°**: {total_samples}
- **ç²¾ç¡®åŒ¹é…æ•°**: {exact_matches}
- **ç²¾ç¡®åŒ¹é…ç‡**: {exact_match_rate:.2%}

## ğŸ“ æäº¤æ–‡ä»¶

**ç»“æœæ–‡ä»¶**: `{result_file.name}`

æ­¤æ–‡ä»¶åŒ…å«ï¼š
- æ¯ä¸ªä»»åŠ¡çš„task_id
- æ¨¡å‹çš„ç­”æ¡ˆï¼ˆmodel_answerï¼‰
- æ¨ç†è½¨è¿¹ï¼ˆreasoning_traceï¼‰

## ğŸš€ å¦‚ä½•æäº¤åˆ°GAIAæ’è¡Œæ¦œ

### æ­¥éª¤1: è®¿é—®GAIAæ’è¡Œæ¦œ

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—®ï¼š
```
https://huggingface.co/spaces/gaia-benchmark/leaderboard
```

### æ­¥éª¤2: å‡†å¤‡æäº¤ä¿¡æ¯

åœ¨æäº¤è¡¨å•ä¸­å¡«å†™ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **Model Nameï¼ˆæ¨¡å‹åç§°ï¼‰**: `{agent_name}`
2. **Model Familyï¼ˆæ¨¡å‹å®¶æ—ï¼‰**: ä¾‹å¦‚ `GPT`, `Claude`, `Qwen` ç­‰
3. **Model Typeï¼ˆæ¨¡å‹ç±»å‹ï¼‰**:
   - `Open-source` (å¼€æº)
   - `Proprietary` (ä¸“æœ‰)
4. **Results Fileï¼ˆç»“æœæ–‡ä»¶ï¼‰**: ä¸Šä¼  `{result_file.name}`

### æ­¥éª¤3: ä¸Šä¼ ç»“æœæ–‡ä»¶

1. ç‚¹å‡» "Choose File" æŒ‰é’®
2. é€‰æ‹©æ–‡ä»¶: `{result_file.absolute()}`
3. ç¡®è®¤æ–‡ä»¶æ ¼å¼ä¸º `.jsonl`

### æ­¥éª¤4: æäº¤

1. æ£€æŸ¥æ‰€æœ‰ä¿¡æ¯æ˜¯å¦æ­£ç¡®
2. ç‚¹å‡» "Submit" æŒ‰é’®
3. ç­‰å¾…è¯„ä¼°ç»“æœï¼ˆé€šå¸¸éœ€è¦å‡ åˆ†é’Ÿï¼‰

## ğŸ“‹ ç»“æœæ–‡ä»¶æ ¼å¼è¯´æ˜

GAIAè¦æ±‚çš„JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰ï¼š

```json
{{"task_id": "xxx", "model_answer": "ç­”æ¡ˆ", "reasoning_trace": "æ¨ç†è¿‡ç¨‹"}}
```

**å­—æ®µè¯´æ˜**ï¼š
- `task_id`: ä»»åŠ¡IDï¼ˆä¸GAIAæ•°æ®é›†å¯¹åº”ï¼‰
- `model_answer`: æ¨¡å‹çš„æœ€ç»ˆç­”æ¡ˆ
- `reasoning_trace`: æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç­”æ¡ˆæ ¼å¼**ï¼š
   - æ•°å­—ï¼šä¸ä½¿ç”¨é€—å·åˆ†éš”ç¬¦ï¼Œä¸ä½¿ç”¨å•ä½ç¬¦å·
   - å­—ç¬¦ä¸²ï¼šä¸ä½¿ç”¨å† è¯ï¼Œä½¿ç”¨å°å†™
   - åˆ—è¡¨ï¼šé€—å·åˆ†éš”ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åˆ—

2. **æ–‡ä»¶å¤§å°**ï¼š
   - ç¡®ä¿æ–‡ä»¶ä¸è¶…è¿‡10MB
   - å¦‚æœæ–‡ä»¶è¿‡å¤§ï¼Œè€ƒè™‘ç§»é™¤reasoning_trace

3. **æäº¤é¢‘ç‡**ï¼š
   - å»ºè®®å…ˆåœ¨å°æ ·æœ¬ä¸Šæµ‹è¯•
   - ç¡®è®¤ç»“æœæ­£ç¡®åå†æäº¤å®Œæ•´è¯„ä¼°

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æŸ¥çœ‹GAIAå®˜æ–¹æ–‡æ¡£ï¼šhttps://huggingface.co/gaia-benchmark
2. åœ¨HuggingFaceè®ºå›æé—®
3. æ£€æŸ¥ç»“æœæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®

---

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**å·¥å…·ç‰ˆæœ¬**: HelloAgents GAIA Evaluation Tool v1.0
"""

        # ä¿å­˜æäº¤è¯´æ˜
        guide_file = output_dir / f"SUBMISSION_GUIDE_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(guide_file, 'w', encoding='utf-8') as f:
            f.write(guide_content)

        print(f"ğŸ“„ æäº¤è¯´æ˜å·²ç”Ÿæˆ: {guide_file}")

    def generate_report(
        self,
        results: Dict[str, Any],
        output_file: Optional[Union[str, Path]] = None
    ) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

        Args:
            results: è¯„ä¼°ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Š
        """
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        agent_name = results.get("agent_name", "Unknown")
        level = results.get("level_filter")
        total_samples = results.get("total_samples", 0)
        exact_matches = results.get("exact_matches", 0)
        partial_matches = results.get("partial_matches", 0)
        exact_match_rate = results.get("exact_match_rate", 0)
        partial_match_rate = results.get("partial_match_rate", 0)
        level_metrics = results.get("level_metrics", {})
        detailed_results = results.get("detailed_results", [])

        # æ„å»ºæŠ¥å‘Š
        report = f"""# GAIAè¯„ä¼°æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}

## ğŸ“Š è¯„ä¼°æ¦‚è§ˆ

- **æ™ºèƒ½ä½“**: {agent_name}
- **éš¾åº¦çº§åˆ«**: {level or 'å…¨éƒ¨'}
- **æ€»æ ·æœ¬æ•°**: {total_samples}
- **ç²¾ç¡®åŒ¹é…æ•°**: {exact_matches}
- **éƒ¨åˆ†åŒ¹é…æ•°**: {partial_matches}
- **ç²¾ç¡®åŒ¹é…ç‡**: {exact_match_rate:.2%}
- **éƒ¨åˆ†åŒ¹é…ç‡**: {partial_match_rate:.2%}

## ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡

### åˆ†çº§å‡†ç¡®ç‡

"""

        # æ·»åŠ åˆ†çº§ç»Ÿè®¡
        for level_name, metrics in level_metrics.items():
            level_num = level_name.replace("Level_", "")
            total = metrics.get("total", 0)
            exact = metrics.get("exact_matches", 0)
            partial = metrics.get("partial_matches", 0)
            exact_rate = metrics.get("exact_match_rate", 0)
            partial_rate = metrics.get("partial_match_rate", 0)

            report += f"- **Level {level_num}**: {exact_rate:.2%} ç²¾ç¡® / {partial_rate:.2%} éƒ¨åˆ† ({exact}/{total})\n"

        # æ·»åŠ æ ·æœ¬è¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰
        report += "\n## ğŸ“ æ ·æœ¬è¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰\n\n"
        report += "| ä»»åŠ¡ID | çº§åˆ« | é¢„æµ‹ç­”æ¡ˆ | æ­£ç¡®ç­”æ¡ˆ | ç²¾ç¡®åŒ¹é… | éƒ¨åˆ†åŒ¹é… |\n"
        report += "|--------|------|----------|----------|----------|----------|\n"

        for i, detail in enumerate(detailed_results[:10]):
            task_id = detail.get("task_id", "")
            level_num = detail.get("level", "")
            predicted = str(detail.get("predicted", ""))[:50]  # é™åˆ¶é•¿åº¦
            expected = str(detail.get("expected", ""))[:50]
            exact = "âœ…" if detail.get("exact_match") else "âŒ"
            partial = "âœ…" if detail.get("partial_match") else "âŒ"

            report += f"| {task_id} | {level_num} | {predicted} | {expected} | {exact} | {partial} |\n"

        # æ·»åŠ å‡†ç¡®ç‡å¯è§†åŒ–
        report += "\n## ğŸ“Š å‡†ç¡®ç‡å¯è§†åŒ–\n\n"
        report += "```\n"
        bar_length = 50
        filled = int(exact_match_rate * bar_length)
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
        report += f"ç²¾ç¡®åŒ¹é…: {bar} {exact_match_rate:.2%}\n"

        filled_partial = int(partial_match_rate * bar_length)
        bar_partial = "â–ˆ" * filled_partial + "â–‘" * (bar_length - filled_partial)
        report += f"éƒ¨åˆ†åŒ¹é…: {bar_partial} {partial_match_rate:.2%}\n"
        report += "```\n"

        # æ·»åŠ å»ºè®®
        report += "\n## ğŸ’¡ å»ºè®®\n\n"
        if exact_match_rate >= 0.9:
            report += "- âœ… è¡¨ç°ä¼˜ç§€ï¼æ™ºèƒ½ä½“åœ¨GAIAåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ã€‚\n"
        elif exact_match_rate >= 0.7:
            report += "- ğŸ‘ è¡¨ç°è‰¯å¥½ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚\n"
            report += "- ğŸ’¡ å»ºè®®ä¼˜åŒ–æç¤ºè¯å’Œæ¨ç†ç­–ç•¥ã€‚\n"
        elif exact_match_rate >= 0.5:
            report += "- âš ï¸ è¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦æ”¹è¿›ã€‚\n"
            report += "- ğŸ’¡ å»ºè®®æ£€æŸ¥å·¥å…·ä½¿ç”¨å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚\n"
        else:
            report += "- âŒ è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦å¤§å¹…æ”¹è¿›ã€‚\n"
            report += "- ğŸ’¡ å»ºè®®ä»ç®€å•çº§åˆ«å¼€å§‹ï¼Œé€æ­¥æå‡ã€‚\n"

        # ä¿å­˜æŠ¥å‘Š
        if output_file is None:
            output_dir = Path("./evaluation_reports")
            output_dir.mkdir(parents=True, exist_ok=True)
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = output_dir / f"gaia_report_{timestamp_str}.md"
        else:
            output_file = Path(output_file)
            output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

        return report

    def get_dataset_info(self, level: Optional[int] = None) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯
        
        Args:
            level: éš¾åº¦çº§åˆ«
            
        Returns:
            æ•°æ®é›†ä¿¡æ¯å­—å…¸
        """
        try:
            dataset = GAIADataset(level=level, local_data_path=self.local_data_path)
            items = dataset.load()
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = dataset.get_statistics()
            level_dist = dataset.get_level_distribution()
            
            return {
                "level": level,
                "total_samples": len(items),
                "level_distribution": level_dist,
                "statistics": stats,
                "sample_keys": list(items[0].keys()) if items else [],
                "levels_available": [1, 2, 3]
            }
        except Exception as e:
            return {"error": str(e)}
    
    def validate_agent(self, agent: Any) -> bool:
        """éªŒè¯æ™ºèƒ½ä½“æ˜¯å¦å…·å¤‡å¿…è¦çš„æ¥å£
        
        Args:
            agent: è¦éªŒè¯çš„æ™ºèƒ½ä½“
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        # æ£€æŸ¥agentæ˜¯å¦æœ‰runæ–¹æ³•
        if not hasattr(agent, 'run'):
            return False
        
        # æ£€æŸ¥runæ–¹æ³•æ˜¯å¦å¯è°ƒç”¨
        if not callable(getattr(agent, 'run')):
            return False
        
        return True
    

