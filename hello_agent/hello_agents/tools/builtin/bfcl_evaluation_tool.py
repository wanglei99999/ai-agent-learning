"""BFCLè¯„ä¼°å·¥å…·

Berkeley Function Calling Leaderboard (BFCL) ä¸€é”®è¯„ä¼°å·¥å…·

æœ¬å·¥å…·å°è£…äº†å®Œæ•´çš„BFCLè¯„ä¼°æµç¨‹ï¼š
1. è‡ªåŠ¨æ£€æŸ¥å’Œå‡†å¤‡BFCLæ•°æ®
2. è¿è¡ŒHelloAgentsè¯„ä¼°
3. å¯¼å‡ºBFCLæ ¼å¼ç»“æœ
4. è°ƒç”¨BFCLå®˜æ–¹è¯„ä¼°å·¥å…·ï¼ˆå¯é€‰ï¼‰
5. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from hello_agents import SimpleAgent, HelloAgentsLLM
    from hello_agents.tools.builtin import BFCLEvaluationTool

    # åˆ›å»ºæ™ºèƒ½ä½“
    llm = HelloAgentsLLM()
    agent = SimpleAgent(name="TestAgent", llm=llm)

    # åˆ›å»ºè¯„ä¼°å·¥å…·
    bfcl_tool = BFCLEvaluationTool()

    # è¿è¡Œè¯„ä¼°ï¼ˆé»˜è®¤ä¼šè¿è¡ŒBFCLå®˜æ–¹è¯„ä¼°ï¼‰
    results = bfcl_tool.run(
        agent=agent,
        category="simple_python",
        max_samples=5
    )

    print(f"å‡†ç¡®ç‡: {results['overall_accuracy']:.2%}")
    # æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆåˆ°: evaluation_reports/bfcl_report_{timestamp}.md
"""

import subprocess
import shutil
import json
from pathlib import Path
from typing import Dict, Any, Optional, List
from ..base import Tool, ToolParameter


class BFCLEvaluationTool(Tool):
    """BFCLä¸€é”®è¯„ä¼°å·¥å…·

    å°è£…äº†å®Œæ•´çš„BFCLè¯„ä¼°æµç¨‹ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„æ¥å£ã€‚

    æ”¯æŒçš„è¯„ä¼°ç±»åˆ«ï¼š
    - simple_python: ç®€å•Pythonå‡½æ•°è°ƒç”¨ï¼ˆ400æ ·æœ¬ï¼‰
    - simple_java: ç®€å•Javaå‡½æ•°è°ƒç”¨ï¼ˆ400æ ·æœ¬ï¼‰
    - simple_javascript: ç®€å•JavaScriptå‡½æ•°è°ƒç”¨ï¼ˆ400æ ·æœ¬ï¼‰
    - multiple: å¤šå‡½æ•°è°ƒç”¨ï¼ˆ240æ ·æœ¬ï¼‰
    - parallel: å¹¶è¡Œå‡½æ•°è°ƒç”¨ï¼ˆ280æ ·æœ¬ï¼‰
    - parallel_multiple: å¹¶è¡Œå¤šå‡½æ•°è°ƒç”¨ï¼ˆ200æ ·æœ¬ï¼‰
    - irrelevance: æ— å…³æ£€æµ‹ï¼ˆ200æ ·æœ¬ï¼‰
    """

    def __init__(self, bfcl_data_dir: Optional[str] = None, project_root: Optional[str] = None):
        """åˆå§‹åŒ–BFCLè¯„ä¼°å·¥å…·

        Args:
            bfcl_data_dir: BFCLæ•°æ®ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼š./temp_gorilla/berkeley-function-call-leaderboard/bfcl_eval/dataï¼‰
            project_root: é¡¹ç›®æ ¹ç›®å½•ï¼ˆé»˜è®¤ï¼šå½“å‰ç›®å½•ï¼‰
        """
        super().__init__(
            name="bfcl_evaluation",
            description=(
                "BFCLä¸€é”®è¯„ä¼°å·¥å…·ã€‚è¯„ä¼°æ™ºèƒ½ä½“çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œæ”¯æŒå¤šä¸ªè¯„ä¼°ç±»åˆ«ã€‚"
                "è‡ªåŠ¨å®Œæˆæ•°æ®åŠ è½½ã€è¯„ä¼°è¿è¡Œã€ç»“æœå¯¼å‡ºå’ŒæŠ¥å‘Šç”Ÿæˆã€‚"
            )
        )

        # è®¾ç½®è·¯å¾„
        self.project_root = Path(project_root) if project_root else Path.cwd()
        if bfcl_data_dir:
            self.bfcl_data_dir = Path(bfcl_data_dir)
        else:
            self.bfcl_data_dir = self.project_root / "temp_gorilla" / "berkeley-function-call-leaderboard" / "bfcl_eval" / "data"

    def get_parameters(self) -> List[ToolParameter]:
        """è·å–å·¥å…·å‚æ•°å®šä¹‰"""
        return [
            ToolParameter(
                name="agent",
                type="object",
                description="è¦è¯„ä¼°çš„æ™ºèƒ½ä½“å®ä¾‹",
                required=True
            ),
            ToolParameter(
                name="category",
                type="string",
                description="è¯„ä¼°ç±»åˆ«ï¼šsimple_python, simple_java, simple_javascript, multiple, parallel, parallel_multiple, irrelevance",
                required=False,
                default="simple_python"
            ),
            ToolParameter(
                name="max_samples",
                type="integer",
                description="è¯„ä¼°æ ·æœ¬æ•°ï¼ˆé»˜è®¤ï¼š5ï¼Œè®¾ä¸º0è¡¨ç¤ºå…¨éƒ¨ï¼‰",
                required=False,
                default=5
            ),
            ToolParameter(
                name="run_official_eval",
                type="boolean",
                description="æ˜¯å¦è¿è¡ŒBFCLå®˜æ–¹è¯„ä¼°",
                required=False,
                default=True
            ),
            ToolParameter(
                name="model_name",
                type="string",
                description="æ¨¡å‹åç§°ï¼ˆç”¨äºBFCLå®˜æ–¹è¯„ä¼°ï¼‰",
                required=False,
                default="Qwen/Qwen3-8B"
            )
        ]

    def run(self, agent: Any, category: str = "simple_python", max_samples: int = 5,
            run_official_eval: bool = True, model_name: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡ŒBFCLè¯„ä¼°

        Args:
            agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“
            category: è¯„ä¼°ç±»åˆ«ï¼ˆé»˜è®¤ï¼šsimple_pythonï¼‰
            max_samples: è¯„ä¼°æ ·æœ¬æ•°ï¼ˆé»˜è®¤ï¼š5ï¼Œè®¾ä¸º0è¡¨ç¤ºå…¨éƒ¨ï¼‰
            run_official_eval: æ˜¯å¦è¿è¡ŒBFCLå®˜æ–¹è¯„ä¼°ï¼ˆé»˜è®¤ï¼šTrueï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºBFCLå®˜æ–¹è¯„ä¼°ï¼Œé»˜è®¤ï¼šQwen/Qwen3-8Bï¼‰

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - overall_accuracy: æ€»ä½“å‡†ç¡®ç‡
            - correct_samples: æ­£ç¡®æ ·æœ¬æ•°
            - total_samples: æ€»æ ·æœ¬æ•°
            - category_metrics: åˆ†ç±»æŒ‡æ ‡
            - detailed_results: è¯¦ç»†ç»“æœ
        """
        from hello_agents.evaluation import BFCLDataset, BFCLEvaluator

        print("\n" + "="*60)
        print("BFCLä¸€é”®è¯„ä¼°")
        print("="*60)
        print(f"\né…ç½®:")
        print(f"   è¯„ä¼°ç±»åˆ«: {category}")
        print(f"   æ ·æœ¬æ•°é‡: {max_samples if max_samples > 0 else 'å…¨éƒ¨'}")
        print(f"   æ™ºèƒ½ä½“: {getattr(agent, 'name', 'Unknown')}")

        # æ­¥éª¤1: æ£€æŸ¥BFCLæ•°æ®
        if not self._check_bfcl_data():
            return self._create_error_result("BFCLæ•°æ®ç›®å½•ä¸å­˜åœ¨")

        # æ­¥éª¤2: è¿è¡ŒHelloAgentsè¯„ä¼°
        print("\n" + "="*60)
        print("æ­¥éª¤1: è¿è¡ŒHelloAgentsè¯„ä¼°")
        print("="*60)

        dataset = BFCLDataset(bfcl_data_dir=str(self.bfcl_data_dir), category=category)
        evaluator = BFCLEvaluator(dataset=dataset, category=category)

        if max_samples > 0:
            results = evaluator.evaluate(agent, max_samples=max_samples)
        else:
            results = evaluator.evaluate(agent, max_samples=None)

        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   å‡†ç¡®ç‡: {results['overall_accuracy']:.2%}")
        print(f"   æ­£ç¡®æ•°: {results['correct_samples']}/{results['total_samples']}")

        # æ­¥éª¤3: å¯¼å‡ºBFCLæ ¼å¼ç»“æœ
        print("\n" + "="*60)
        print("æ­¥éª¤2: å¯¼å‡ºBFCLæ ¼å¼ç»“æœ")
        print("="*60)

        output_dir = self.project_root / "evaluation_results" / "bfcl_official"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"BFCL_v4_{category}_result.json"

        evaluator.export_to_bfcl_format(results, output_file)

        # æ­¥éª¤4: è¿è¡ŒBFCLå®˜æ–¹è¯„ä¼°ï¼ˆå¯é€‰ï¼‰
        if run_official_eval:
            if not model_name:
                model_name = "Qwen/Qwen3-8B"

            self._run_official_evaluation(output_file, model_name, category)

        # æ­¥éª¤5: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        print("\n" + "="*60)
        print("æ­¥éª¤3: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        print("="*60)

        # æ·»åŠ æ™ºèƒ½ä½“å’Œç±»åˆ«ä¿¡æ¯åˆ°ç»“æœä¸­
        results['agent_name'] = getattr(agent, 'name', 'Unknown')
        results['category'] = category

        self.generate_report(results)

        return results

    def _check_bfcl_data(self) -> bool:
        """æ£€æŸ¥BFCLæ•°æ®æ˜¯å¦å­˜åœ¨"""
        if not self.bfcl_data_dir.exists():
            print(f"\nâŒ BFCLæ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.bfcl_data_dir}")
            print(f"\nè¯·å…ˆå…‹éš†BFCLä»“åº“ï¼š")
            print(f"   git clone --depth 1 https://github.com/ShishirPatil/gorilla.git temp_gorilla")
            return False
        return True

    def _run_official_evaluation(self, source_file: Path, model_name: str, category: str):
        """è¿è¡ŒBFCLå®˜æ–¹è¯„ä¼°"""
        print("\n" + "="*60)
        print("æ­¥éª¤3: è¿è¡ŒBFCLå®˜æ–¹è¯„ä¼°")
        print("="*60)

        # å¤åˆ¶ç»“æœæ–‡ä»¶åˆ°BFCLç»“æœç›®å½•
        safe_model_name = model_name.replace("/", "_")
        result_dir = self.project_root / "result" / safe_model_name
        result_dir.mkdir(parents=True, exist_ok=True)

        target_file = result_dir / f"BFCL_v4_{category}_result.json"
        shutil.copy(source_file, target_file)

        print(f"\nâœ… ç»“æœæ–‡ä»¶å·²å¤åˆ¶åˆ°:")
        print(f"   {target_file}")

        # è¿è¡ŒBFCLè¯„ä¼°
        try:
            import os
            os.environ['PYTHONUTF8'] = '1'

            cmd = [
                "bfcl", "evaluate",
                "--model", model_name,
                "--test-category", category,
                "--partial-eval"
            ]

            print(f"\nğŸ”„ è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")

            result = subprocess.run(
                cmd,
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                encoding='utf-8'
            )

            if result.stdout:
                print(result.stdout)

            if result.returncode != 0:
                print(f"\nâŒ BFCLè¯„ä¼°å¤±è´¥:")
                if result.stderr:
                    print(result.stderr)
            else:
                self._show_official_results(model_name, category)

        except FileNotFoundError:
            print("\nâŒ æœªæ‰¾åˆ°bfclå‘½ä»¤")
            print("   è¯·å…ˆå®‰è£…: pip install bfcl-eval")
        except Exception as e:
            print(f"\nâŒ è¿è¡ŒBFCLè¯„ä¼°æ—¶å‡ºé”™: {e}")

    def _show_official_results(self, model_name: str, category: str):
        """å±•ç¤ºBFCLå®˜æ–¹è¯„ä¼°ç»“æœ"""
        print("\n" + "="*60)
        print("BFCLå®˜æ–¹è¯„ä¼°ç»“æœ")
        print("="*60)

        # CSVæ–‡ä»¶
        csv_file = self.project_root / "score" / "data_non_live.csv"

        if csv_file.exists():
            print(f"\nğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»:")
            with open(csv_file, 'r', encoding='utf-8') as f:
                content = f.read()
                print(content)

        # è¯¦ç»†è¯„åˆ†æ–‡ä»¶
        safe_model_name = model_name.replace("/", "_")
        score_file = self.project_root / "score" / safe_model_name / "non_live" / f"BFCL_v4_{category}_score.json"

        if score_file.exists():
            print(f"\nğŸ“ è¯¦ç»†è¯„åˆ†æ–‡ä»¶:")
            print(f"   {score_file}")

            with open(score_file, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                summary = json.loads(first_line)
                print(f"\nğŸ¯ æœ€ç»ˆç»“æœ:")
                print(f"   å‡†ç¡®ç‡: {summary['accuracy']:.2%}")
                print(f"   æ­£ç¡®æ•°: {summary['correct_count']}/{summary['total_count']}")

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "error": error_message,
            "overall_accuracy": 0.0,
            "correct_samples": 0,
            "total_samples": 0,
            "category_metrics": {},
            "detailed_results": []
        }

    def generate_report(self, results: Dict[str, Any], output_file: Optional[str] = None) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼ševaluation_reports/bfcl_report_{timestamp}.mdï¼‰

        Returns:
            æŠ¥å‘Šå†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        from datetime import datetime

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        report = f"""# BFCLè¯„ä¼°æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}

## ğŸ“Š è¯„ä¼°æ¦‚è§ˆ

- **æ™ºèƒ½ä½“**: {results.get('agent_name', 'Unknown')}
- **è¯„ä¼°ç±»åˆ«**: {results.get('category', 'Unknown')}
- **æ€»ä½“å‡†ç¡®ç‡**: {results['overall_accuracy']:.2%}
- **æ­£ç¡®æ ·æœ¬æ•°**: {results['correct_samples']}/{results['total_samples']}

## ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡

"""

        # æ·»åŠ åˆ†ç±»æŒ‡æ ‡
        if 'category_metrics' in results and results['category_metrics']:
            report += "### åˆ†ç±»å‡†ç¡®ç‡\n\n"
            for category, metrics in results['category_metrics'].items():
                accuracy = metrics.get('accuracy', 0.0)
                correct = metrics.get('correct', 0)
                total = metrics.get('total', 0)
                report += f"- **{category}**: {accuracy:.2%} ({correct}/{total})\n"
            report += "\n"

        # æ·»åŠ æ ·æœ¬è¯¦æƒ…
        if 'detailed_results' in results and results['detailed_results']:
            report += "## ğŸ“ æ ·æœ¬è¯¦æƒ…\n\n"
            report += "| æ ·æœ¬ID | é—®é¢˜ | é¢„æµ‹ç»“æœ | æ­£ç¡®ç­”æ¡ˆ | æ˜¯å¦æ­£ç¡® |\n"
            report += "|--------|------|----------|----------|----------|\n"

            for detail in results['detailed_results'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                sample_id = detail.get('sample_id', 'N/A')

                # æå–é—®é¢˜æ–‡æœ¬
                question = detail.get('question', 'N/A')
                if isinstance(question, list) and len(question) > 0:
                    if isinstance(question[0], list) and len(question[0]) > 0:
                        if isinstance(question[0][0], dict) and 'content' in question[0][0]:
                            question = question[0][0]['content']
                question_str = str(question)
                if len(question_str) > 60:
                    question_str = question_str[:60] + "..."

                # æå–é¢„æµ‹ç»“æœï¼ˆå­—æ®µåæ˜¯predictedï¼‰
                prediction = detail.get('predicted', 'N/A')
                if prediction and prediction != 'N/A':
                    pred_str = str(prediction)
                    if len(pred_str) > 40:
                        pred_str = pred_str[:40] + "..."
                else:
                    pred_str = "N/A"

                # æå–æ­£ç¡®ç­”æ¡ˆï¼ˆå­—æ®µåæ˜¯expectedï¼‰
                ground_truth = detail.get('expected', 'N/A')
                if ground_truth and ground_truth != 'N/A':
                    gt_str = str(ground_truth)
                    if len(gt_str) > 40:
                        gt_str = gt_str[:40] + "..."
                else:
                    gt_str = "N/A"

                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®ï¼ˆå­—æ®µåæ˜¯successï¼‰
                is_correct = "âœ…" if detail.get('success', False) else "âŒ"

                report += f"| {sample_id} | {question_str} | {pred_str} | {gt_str} | {is_correct} |\n"

            if len(results['detailed_results']) > 10:
                report += f"\n*æ˜¾ç¤ºå‰10ä¸ªæ ·æœ¬ï¼Œå…±{len(results['detailed_results'])}ä¸ªæ ·æœ¬*\n"
            report += "\n"

        # æ·»åŠ å¯è§†åŒ–ï¼ˆASCIIå›¾è¡¨ï¼‰
        report += "## ğŸ“Š å‡†ç¡®ç‡å¯è§†åŒ–\n\n"
        report += "```\n"
        accuracy = results['overall_accuracy']
        bar_length = int(accuracy * 50)
        bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
        report += f"å‡†ç¡®ç‡: {bar} {accuracy:.2%}\n"
        report += "```\n\n"

        # æ·»åŠ å»ºè®®
        report += "## ğŸ’¡ å»ºè®®\n\n"
        if accuracy >= 0.9:
            report += "- âœ… è¡¨ç°ä¼˜ç§€ï¼æ™ºèƒ½ä½“åœ¨å·¥å…·è°ƒç”¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚\n"
        elif accuracy >= 0.7:
            report += "- âš ï¸ è¡¨ç°è‰¯å¥½ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚å»ºè®®æ£€æŸ¥é”™è¯¯æ ·æœ¬ï¼Œä¼˜åŒ–æç¤ºè¯ã€‚\n"
        else:
            report += "- âŒ è¡¨ç°éœ€è¦æ”¹è¿›ã€‚å»ºè®®ï¼š\n"
            report += "  1. æ£€æŸ¥æ™ºèƒ½ä½“çš„å·¥å…·è°ƒç”¨é€»è¾‘\n"
            report += "  2. ä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯\n"
            report += "  3. å¢åŠ æ›´å¤šè®­ç»ƒæ ·æœ¬\n"

        # ä¿å­˜æŠ¥å‘Š
        if output_file is None:
            timestamp_file = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = self.project_root / "evaluation_reports"
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / f"bfcl_report_{timestamp_file}.md"
        else:
            output_file = Path(output_file)
            output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"\nğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

        return report