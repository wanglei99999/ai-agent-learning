# LLM æ¨¡å—è¯¦è§£ (`core/llm.py`)

> æ·±å…¥ç†è§£ HelloAgents çš„ LLM ç»Ÿä¸€æ¥å£å®ç°

---

## ğŸ“š ç›®å½•

- [æ¨¡å—æ¦‚è¿°](#æ¨¡å—æ¦‚è¿°)
- [æ ¸å¿ƒè®¾è®¡ç†å¿µ](#æ ¸å¿ƒè®¾è®¡ç†å¿µ)
- [ç±»ç»“æ„åˆ†æ](#ç±»ç»“æ„åˆ†æ)
- [å…³é”®æ–¹æ³•è¯¦è§£](#å…³é”®æ–¹æ³•è¯¦è§£)
- [å‚å•†æ”¯æŒæœºåˆ¶](#å‚å•†æ”¯æŒæœºåˆ¶)
- [å®æˆ˜ç¤ºä¾‹](#å®æˆ˜ç¤ºä¾‹)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ¨¡å—æ¦‚è¿°

### æ–‡ä»¶ä½ç½®
`hello_agents/core/llm.py`

### ä¸»è¦åŠŸèƒ½
- æä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£
- æ”¯æŒå¤šä¸ª LLM å‚å•†ï¼ˆé€šè¿‡ OpenAI å…¼å®¹æ¥å£ï¼‰
- è‡ªåŠ¨æ£€æµ‹å’Œé…ç½®ä¸åŒçš„ LLM æä¾›å•†
- æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§å“åº”æ¨¡å¼

### æ ¸å¿ƒç±»
- `HelloAgentsLLM` - LLM å®¢æˆ·ç«¯å°è£…ç±»

---

## æ ¸å¿ƒè®¾è®¡ç†å¿µ

### 1. å‚æ•°ä¼˜å…ˆï¼Œç¯å¢ƒå˜é‡å…œåº•

```python
# ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°
self.model = model or os.getenv("LLM_MODEL_ID")
self.api_key = api_key or os.getenv("LLM_API_KEY")
self.base_url = base_url or os.getenv("LLM_BASE_URL")
```

**è®¾è®¡æ„å›¾**ï¼š
- çµæ´»æ€§ï¼šå…è®¸è¿è¡Œæ—¶åŠ¨æ€é…ç½®
- ä¾¿åˆ©æ€§ï¼šæ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼Œé¿å…ç¡¬ç¼–ç 
- ä¼˜å…ˆçº§ï¼šæ˜¾å¼å‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼

### 2. æµå¼å“åº”ä¸ºé»˜è®¤

```python
def think(self, messages, temperature=None) -> Iterator[str]:
    response = self._client.chat.completions.create(
        model=self.model,
        messages=messages,
        stream=True,  # é»˜è®¤å¼€å¯æµå¼
    )
    for chunk in response:
        content = chunk.choices[0].delta.content or ""
        if content:
            yield content
```

**ä¸ºä»€ä¹ˆé»˜è®¤æµå¼ï¼Ÿ**
- âœ… æ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼ˆå®æ—¶åé¦ˆï¼‰
- âœ… é™ä½æ„ŸçŸ¥å»¶è¿Ÿ
- âœ… é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆ

### 3. OpenAI å…¼å®¹æ¥å£çš„ç§˜å¯†

**æ ¸å¿ƒåŸç†**ï¼šé€šè¿‡ä¿®æ”¹ `base_url` åˆ‡æ¢ä¸åŒå‚å•†

```python
client = OpenAI(
    api_key=self.api_key,
    base_url=self.base_url,  # è¿™æ˜¯å…³é”®ï¼
    timeout=self.timeout
)
```

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**
- ğŸ¯ OpenAI API æ ¼å¼å·²æˆä¸ºè¡Œä¸šæ ‡å‡†
- ğŸ¯ å¤§éƒ¨åˆ†å‚å•†ä¸»åŠ¨å…¼å®¹ OpenAI æ¥å£
- ğŸ¯ ä¸€å¥—ä»£ç æ”¯æŒå¤šä¸ªå‚å•†

---

## ç±»ç»“æ„åˆ†æ

### åˆå§‹åŒ–æ–¹æ³• `__init__`

```python
def __init__(
    self,
    model: Optional[str] = None,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    provider: Optional[SUPPORTED_PROVIDERS] = None,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    timeout: Optional[int] = None,
    **kwargs
):
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model` | str | None | æ¨¡å‹åç§°ï¼Œæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å– |
| `api_key` | str | None | APIå¯†é’¥ |
| `base_url` | str | None | æœåŠ¡åœ°å€ |
| `provider` | str | None | æä¾›å•†åç§°ï¼Œæ”¯æŒè‡ªåŠ¨æ£€æµ‹ |
| `temperature` | float | 0.7 | æ¸©åº¦å‚æ•°ï¼ˆ0-2ï¼‰ï¼Œæ§åˆ¶éšæœºæ€§ |
| `max_tokens` | int | None | æœ€å¤§ç”Ÿæˆtokenæ•° |
| `timeout` | int | 60 | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |

#### åˆå§‹åŒ–æµç¨‹

```mermaid
graph TD
    A[å¼€å§‹åˆå§‹åŒ–] --> B[è¯»å–å‚æ•°/ç¯å¢ƒå˜é‡]
    B --> C{provider æ˜¯å¦æŒ‡å®š?}
    C -->|æ˜¯| D[ä½¿ç”¨æŒ‡å®š provider]
    C -->|å¦| E[è‡ªåŠ¨æ£€æµ‹ provider]
    E --> F[_auto_detect_provider]
    D --> G[è§£æå‡­è¯]
    F --> G
    G --> H[_resolve_credentials]
    H --> I{model æ˜¯å¦æŒ‡å®š?}
    I -->|å¦| J[è·å–é»˜è®¤æ¨¡å‹]
    I -->|æ˜¯| K[éªŒè¯å¿…è¦å‚æ•°]
    J --> K
    K --> L[åˆ›å»º OpenAI å®¢æˆ·ç«¯]
    L --> M[åˆå§‹åŒ–å®Œæˆ]
```

---

## å…³é”®æ–¹æ³•è¯¦è§£

### 1. è‡ªåŠ¨æ£€æµ‹æä¾›å•† `_auto_detect_provider`

è¿™æ˜¯ä¸€ä¸ªéå¸¸æ™ºèƒ½çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šç§ç­–ç•¥è‡ªåŠ¨è¯†åˆ« LLM æä¾›å•†ã€‚

#### æ£€æµ‹ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

```python
def _auto_detect_provider(self, api_key, base_url) -> str:
    # ç­–ç•¥1: æ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
    if os.getenv("OPENAI_API_KEY"):
        return "openai"
    if os.getenv("DEEPSEEK_API_KEY"):
        return "deepseek"
    # ... å…¶ä»–å‚å•†
    
    # ç­–ç•¥2: æ ¹æ® API å¯†é’¥æ ¼å¼åˆ¤æ–­
    if actual_api_key.startswith("ms-"):
        return "modelscope"
    elif actual_api_key.startswith("sk-"):
        # å¯èƒ½æ˜¯ OpenAIã€DeepSeek æˆ– Kimi
        pass
    
    # ç­–ç•¥3: æ ¹æ® base_url åˆ¤æ–­
    if "api.deepseek.com" in base_url_lower:
        return "deepseek"
    elif "dashscope.aliyuncs.com" in base_url_lower:
        return "qwen"
    
    # ç­–ç•¥4: é»˜è®¤è¿”å› auto
    return "auto"
```

#### æ£€æµ‹æµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹æ£€æµ‹] --> B{ç¯å¢ƒå˜é‡å­˜åœ¨?}
    B -->|æ˜¯| C[è¿”å›å¯¹åº” provider]
    B -->|å¦| D{API Key æ ¼å¼åŒ¹é…?}
    D -->|æ˜¯| E[è¿”å›å¯¹åº” provider]
    D -->|å¦| F{base_url åŒ…å«ç‰¹å¾?}
    F -->|æ˜¯| G[è¿”å›å¯¹åº” provider]
    F -->|å¦| H{æ˜¯æœ¬åœ°éƒ¨ç½²?}
    H -->|æ˜¯| I[æ£€æµ‹ç«¯å£/æœåŠ¡ç±»å‹]
    H -->|å¦| J[è¿”å› auto]
```

#### å­¦ä¹ è¦ç‚¹

- [ ] ç†è§£å¤šç­–ç•¥æ£€æµ‹çš„ä¼˜å…ˆçº§
- [ ] æŒæ¡ä¸åŒå‚å•†çš„ç‰¹å¾è¯†åˆ«æ–¹æ³•
- [ ] äº†è§£æœ¬åœ°éƒ¨ç½²çš„æ£€æµ‹é€»è¾‘
- [ ] å­¦ä¹ å¦‚ä½•æ‰©å±•æ”¯æŒæ–°å‚å•†

---

### 2. è§£æå‡­è¯ `_resolve_credentials`

æ ¹æ®æ£€æµ‹åˆ°çš„ providerï¼Œè¿”å›å¯¹åº”çš„ API å¯†é’¥å’Œ base_urlã€‚

#### ä»£ç ç»“æ„

```python
def _resolve_credentials(self, api_key, base_url) -> tuple[str, str]:
    if self.provider == "openai":
        resolved_api_key = api_key or os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
        resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api.openai.com/v1"
        return resolved_api_key, resolved_base_url
    
    elif self.provider == "deepseek":
        resolved_api_key = api_key or os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_API_KEY")
        resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api.deepseek.com"
        return resolved_api_key, resolved_base_url
    
    # ... å…¶ä»–å‚å•†
```

#### å‚å•†é…ç½®è¡¨

| Provider | ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§ | é»˜è®¤ base_url |
|----------|---------------|---------------|
| openai | OPENAI_API_KEY â†’ LLM_API_KEY | https://api.openai.com/v1 |
| deepseek | DEEPSEEK_API_KEY â†’ LLM_API_KEY | https://api.deepseek.com |
| qwen | DASHSCOPE_API_KEY â†’ LLM_API_KEY | https://dashscope.aliyuncs.com/compatible-mode/v1 |
| kimi | KIMI_API_KEY â†’ MOONSHOT_API_KEY â†’ LLM_API_KEY | https://api.moonshot.cn/v1 |
| zhipu | ZHIPU_API_KEY â†’ GLM_API_KEY â†’ LLM_API_KEY | https://open.bigmodel.cn/api/paas/v4 |
| ollama | OLLAMA_API_KEY â†’ LLM_API_KEY (é»˜è®¤"ollama") | http://localhost:11434/v1 |

#### å­¦ä¹ è¦ç‚¹

- [ ] ç†è§£ç¯å¢ƒå˜é‡çš„å›é€€æœºåˆ¶
- [ ] æŒæ¡ä¸åŒå‚å•†çš„é»˜è®¤é…ç½®
- [ ] äº†è§£æœ¬åœ°éƒ¨ç½²çš„ç‰¹æ®Šå¤„ç†

---

### 3. è·å–é»˜è®¤æ¨¡å‹ `_get_default_model`

å½“ç”¨æˆ·æœªæŒ‡å®šæ¨¡å‹æ—¶ï¼Œæ ¹æ® provider è¿”å›åˆé€‚çš„é»˜è®¤æ¨¡å‹ã€‚

```python
def _get_default_model(self) -> str:
    if self.provider == "openai":
        return "gpt-3.5-turbo"
    elif self.provider == "deepseek":
        return "deepseek-chat"
    elif self.provider == "qwen":
        return "qwen-plus"
    # ... å…¶ä»–å‚å•†
    else:
        # auto æ¨¡å¼ï¼šæ ¹æ® base_url æ™ºèƒ½æ¨æ–­
        base_url = os.getenv("LLM_BASE_URL", "")
        if "deepseek" in base_url.lower():
            return "deepseek-chat"
        # ...
        return "gpt-3.5-turbo"  # æœ€ç»ˆé»˜è®¤å€¼
```

#### é»˜è®¤æ¨¡å‹è¡¨

| Provider | é»˜è®¤æ¨¡å‹ | è¯´æ˜ |
|----------|---------|------|
| openai | gpt-3.5-turbo | æ€§ä»·æ¯”é«˜ |
| deepseek | deepseek-chat | é€šç”¨å¯¹è¯æ¨¡å‹ |
| qwen | qwen-plus | é€šä¹‰åƒé—®å¢å¼ºç‰ˆ |
| kimi | moonshot-v1-8k | 8K ä¸Šä¸‹æ–‡ç‰ˆæœ¬ |
| zhipu | glm-4 | æ™ºè°±æœ€æ–°æ¨¡å‹ |
| ollama | llama3.2 | å¸¸ç”¨å¼€æºæ¨¡å‹ |

---

### 4. æµå¼è°ƒç”¨ `think`

æ ¸å¿ƒæ–¹æ³•ï¼Œå®ç°æµå¼ LLM è°ƒç”¨ã€‚

```python
def think(self, messages: list[dict[str, str]], temperature: Optional[float] = None) -> Iterator[str]:
    print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
    try:
        # åˆ›å»ºæµå¼è¯·æ±‚
        response = self._client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature if temperature is not None else self.temperature,
            max_tokens=self.max_tokens,
            stream=True,  # å…³é”®ï¼šå¼€å¯æµå¼
        )

        # å¤„ç†æµå¼å“åº”
        print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
        for chunk in response:
            content = chunk.choices[0].delta.content or ""
            if content:
                print(content, end="", flush=True)
                yield content  # ç”Ÿæˆå™¨æ¨¡å¼
        print()  # æ¢è¡Œ

    except Exception as e:
        print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise HelloAgentsException(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
```

#### æµå¼å“åº”åŸç†

```mermaid
graph LR
    A[å‘èµ·è¯·æ±‚] --> B[stream=True]
    B --> C[æ¥æ”¶ç¬¬ä¸€ä¸ª chunk]
    C --> D[æå– delta.content]
    D --> E[yield è¿”å›]
    E --> F{è¿˜æœ‰ chunk?}
    F -->|æ˜¯| C
    F -->|å¦| G[ç»“æŸ]
```

#### å…³é”®æŠ€æœ¯ç‚¹

**1. ç”Ÿæˆå™¨æ¨¡å¼ (Generator)**
```python
def think(...) -> Iterator[str]:
    for chunk in response:
        yield content  # ä½¿ç”¨ yield è€Œé return
```

**ä¼˜åŠ¿**ï¼š
- å†…å­˜é«˜æ•ˆï¼šä¸éœ€è¦ç­‰å¾…å®Œæ•´å“åº”
- å®æ—¶è¾“å‡ºï¼šè¾¹æ¥æ”¶è¾¹å¤„ç†
- å¯ä¸­æ–­ï¼šå¯ä»¥æå‰åœæ­¢

**2. æµå¼æ•°æ®æå–**
```python
content = chunk.choices[0].delta.content or ""
```

**æ³¨æ„**ï¼š
- æµå¼å“åº”ä½¿ç”¨ `delta.content`ï¼ˆå¢é‡ï¼‰
- éæµå¼å“åº”ä½¿ç”¨ `message.content`ï¼ˆå®Œæ•´ï¼‰
- éœ€è¦å¤„ç† `None` çš„æƒ…å†µ

**3. å®æ—¶æ‰“å°**
```python
print(content, end="", flush=True)
```

- `end=""` - ä¸æ¢è¡Œ
- `flush=True` - ç«‹å³åˆ·æ–°ç¼“å†²åŒº

#### å­¦ä¹ è¦ç‚¹

- [ ] ç†è§£ç”Ÿæˆå™¨æ¨¡å¼çš„ä¼˜åŠ¿
- [ ] æŒæ¡æµå¼å“åº”çš„æ•°æ®ç»“æ„
- [ ] äº†è§£å¦‚ä½•å¤„ç†æµå¼è¾“å‡º
- [ ] å­¦ä¹ é”™è¯¯å¤„ç†æœºåˆ¶

---

### 5. éæµå¼è°ƒç”¨ `invoke`

é€‚ç”¨äºä¸éœ€è¦æµå¼è¾“å‡ºçš„åœºæ™¯ã€‚

```python
def invoke(self, messages: list[dict[str, str]], **kwargs) -> str:
    try:
        response = self._client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=kwargs.get('temperature', self.temperature),
            max_tokens=kwargs.get('max_tokens', self.max_tokens),
            **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_tokens']}
        )
        return response.choices[0].message.content  # ç›´æ¥è¿”å›å®Œæ•´å†…å®¹
    except Exception as e:
        raise HelloAgentsException(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
```

#### ä¸æµå¼è°ƒç”¨çš„åŒºåˆ«

| ç‰¹æ€§ | æµå¼ (think) | éæµå¼ (invoke) |
|------|-------------|----------------|
| è¿”å›ç±»å‹ | `Iterator[str]` | `str` |
| å“åº”æ–¹å¼ | é€å—è¿”å› | ä¸€æ¬¡æ€§è¿”å› |
| ç”¨æˆ·ä½“éªŒ | å®æ—¶åé¦ˆ | ç­‰å¾…å®Œæ•´ç»“æœ |
| å†…å­˜å ç”¨ | ä½ | é«˜ï¼ˆé•¿æ–‡æœ¬ï¼‰ |
| é€‚ç”¨åœºæ™¯ | å¯¹è¯ã€é•¿æ–‡æœ¬ç”Ÿæˆ | çŸ­æ–‡æœ¬ã€éœ€è¦å®Œæ•´ç»“æœçš„åœºæ™¯ |

---

## å‚å•†æ”¯æŒæœºåˆ¶

### å·²æ”¯æŒçš„å‚å•†

#### 1. OpenAI
```python
# é…ç½®ç¤ºä¾‹
llm = HelloAgentsLLM(
    provider="openai",
    api_key="sk-...",
    model="gpt-4"
)
```

#### 2. DeepSeek
```python
# é…ç½®ç¤ºä¾‹
llm = HelloAgentsLLM(
    provider="deepseek",
    api_key="sk-...",
    model="deepseek-chat"
)
```

#### 3. é€šä¹‰åƒé—® (Qwen)
```python
# é…ç½®ç¤ºä¾‹
llm = HelloAgentsLLM(
    provider="qwen",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    model="qwen-plus"
)
```

#### 4. Ollama (æœ¬åœ°éƒ¨ç½²)
```python
# é…ç½®ç¤ºä¾‹
llm = HelloAgentsLLM(
    provider="ollama",
    api_key="ollama",  # æœ¬åœ°éƒ¨ç½²é€šå¸¸ä¸éœ€è¦çœŸå® key
    base_url="http://localhost:11434/v1",
    model="llama3.2"
)
```

### å¦‚ä½•æ·»åŠ æ–°å‚å•†æ”¯æŒï¼Ÿ

#### æ­¥éª¤ 1: æ·»åŠ åˆ° SUPPORTED_PROVIDERS
```python
SUPPORTED_PROVIDERS = Literal[
    "openai",
    "deepseek",
    # ... ç°æœ‰å‚å•†
    "new_provider",  # æ–°å¢
]
```

#### æ­¥éª¤ 2: åœ¨ _auto_detect_provider æ·»åŠ æ£€æµ‹é€»è¾‘
```python
def _auto_detect_provider(self, api_key, base_url):
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if os.getenv("NEW_PROVIDER_API_KEY"):
        return "new_provider"
    
    # æ£€æŸ¥ base_url
    if "new-provider.com" in base_url_lower:
        return "new_provider"
```

#### æ­¥éª¤ 3: åœ¨ _resolve_credentials æ·»åŠ é…ç½®
```python
def _resolve_credentials(self, api_key, base_url):
    # ... ç°æœ‰ä»£ç 
    elif self.provider == "new_provider":
        resolved_api_key = api_key or os.getenv("NEW_PROVIDER_API_KEY") or os.getenv("LLM_API_KEY")
        resolved_base_url = base_url or "https://api.new-provider.com/v1"
        return resolved_api_key, resolved_base_url
```

#### æ­¥éª¤ 4: åœ¨ _get_default_model æ·»åŠ é»˜è®¤æ¨¡å‹
```python
def _get_default_model(self):
    # ... ç°æœ‰ä»£ç 
    elif self.provider == "new_provider":
        return "new-provider-default-model"
```

---

## å®æˆ˜ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨

```python
from hello_agents.core.llm import HelloAgentsLLM

# åˆ›å»º LLM å®ä¾‹
llm = HelloAgentsLLM(
    provider="deepseek",
    api_key="your-api-key",
    model="deepseek-chat",
    temperature=0.7
)

# å‡†å¤‡æ¶ˆæ¯
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯ Agentï¼Ÿ"}
]

# æµå¼è°ƒç”¨
for chunk in llm.think(messages):
    print(chunk, end="", flush=True)

# éæµå¼è°ƒç”¨
response = llm.invoke(messages)
print(response)
```

### ç¤ºä¾‹ 2: ä½¿ç”¨ç¯å¢ƒå˜é‡

```bash
# .env æ–‡ä»¶
LLM_API_KEY=sk-...
LLM_BASE_URL=https://api.deepseek.com
LLM_MODEL_ID=deepseek-chat
```

```python
# Python ä»£ç 
from hello_agents.core.llm import HelloAgentsLLM

# è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
llm = HelloAgentsLLM()  # æ— éœ€ä¼ å‚

messages = [{"role": "user", "content": "Hello"}]
response = llm.invoke(messages)
```

### ç¤ºä¾‹ 3: æœ¬åœ° Ollama

```python
llm = HelloAgentsLLM(
    provider="ollama",
    base_url="http://localhost:11434/v1",
    model="llama3.2",
    api_key="ollama"  # Ollama ä¸éœ€è¦çœŸå® key
)

messages = [{"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ Python"}]
for chunk in llm.think(messages):
    print(chunk, end="")
```

### ç¤ºä¾‹ 4: è‡ªå®šä¹‰é…ç½®

```python
llm = HelloAgentsLLM(
    provider="custom",
    api_key="your-key",
    base_url="https://your-custom-endpoint.com/v1",
    model="your-model",
    temperature=0.9,
    max_tokens=2000,
    timeout=120
)
```

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ API è°ƒç”¨å¤±è´¥ï¼Ÿ

**æ£€æŸ¥æ¸…å•**ï¼š
- [ ] API Key æ˜¯å¦æ­£ç¡®
- [ ] base_url æ˜¯å¦æ­£ç¡®ï¼ˆæ³¨æ„æœ«å°¾æ˜¯å¦æœ‰ `/v1`ï¼‰
- [ ] æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
- [ ] ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
- [ ] æ˜¯å¦æœ‰è¶³å¤Ÿçš„é…é¢

**è°ƒè¯•æ–¹æ³•**ï¼š
```python
# æ‰“å°é…ç½®ä¿¡æ¯
print(f"Provider: {llm.provider}")
print(f"Model: {llm.model}")
print(f"Base URL: {llm.base_url}")
print(f"API Key: {llm.api_key[:10]}...")  # åªæ‰“å°å‰10ä½
```

### Q2: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„æ¨¡å‹ï¼Ÿ

```python
# æ–¹æ³•1: åˆå§‹åŒ–æ—¶æŒ‡å®š
llm = HelloAgentsLLM(model="gpt-4")

# æ–¹æ³•2: è¿è¡Œæ—¶ä¿®æ”¹
llm.model = "gpt-4-turbo"
```

### Q3: æµå¼è¾“å‡ºå¦‚ä½•æ”¶é›†å®Œæ•´å“åº”ï¼Ÿ

```python
full_response = ""
for chunk in llm.think(messages):
    full_response += chunk
    print(chunk, end="", flush=True)

print(f"\nå®Œæ•´å“åº”: {full_response}")
```

### Q4: å¦‚ä½•è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Ÿ

```python
llm = HelloAgentsLLM(
    timeout=120  # 120ç§’è¶…æ—¶
)
```

### Q5: æœ¬åœ°æ¨¡å‹å¦‚ä½•é…ç½®ï¼Ÿ

```python
# Ollama
llm = HelloAgentsLLM(
    provider="ollama",
    base_url="http://localhost:11434/v1",
    model="llama3.2"
)

# vLLM
llm = HelloAgentsLLM(
    provider="vllm",
    base_url="http://localhost:8000/v1",
    model="meta-llama/Llama-2-7b-chat-hf"
)
```

---

## å­¦ä¹ æ£€æŸ¥æ¸…å•

### åŸºç¡€ç†è§£
- [ ] ç†è§£ OpenAI å…¼å®¹æ¥å£çš„åŸç†
- [ ] æŒæ¡å‚æ•°ä¼˜å…ˆçº§æœºåˆ¶
- [ ] äº†è§£æµå¼å’Œéæµå¼çš„åŒºåˆ«
- [ ] ç†è§£ provider è‡ªåŠ¨æ£€æµ‹é€»è¾‘

### è¿›é˜¶æŒæ¡
- [ ] èƒ½å¤Ÿæ·»åŠ æ–°çš„ LLM å‚å•†æ”¯æŒ
- [ ] ç†è§£ç”Ÿæˆå™¨æ¨¡å¼çš„å®ç°
- [ ] æŒæ¡é”™è¯¯å¤„ç†æœºåˆ¶
- [ ] èƒ½å¤Ÿè‡ªå®šä¹‰é…ç½®å’Œæ‰©å±•

### å®æˆ˜åº”ç”¨
- [ ] å®Œæˆè‡³å°‘3ä¸ªä¸åŒå‚å•†çš„é…ç½®
- [ ] å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ LLM åŒ…è£…ç±»
- [ ] å¤„ç†è¿‡æµå¼è¾“å‡ºçš„è¾¹ç•Œæƒ…å†µ
- [ ] ä¼˜åŒ–è¿‡ LLM è°ƒç”¨çš„æ€§èƒ½

---

**ä¸‹ä¸€æ­¥å­¦ä¹ **: [æ¶ˆæ¯ç³»ç»Ÿè¯¦è§£](./02_æ¶ˆæ¯ç³»ç»Ÿè¯¦è§£.md)
